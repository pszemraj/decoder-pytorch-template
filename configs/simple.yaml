# Simple Llama configuration
run_dir: runs/simple
seed: null

# Model
num_tokens: 256 # vocab size (character modeling)
dim: 512 # hidden size
depth: 16 # number of layers
heads: 8 # attention heads
dim_head: 64 # head dimension
tied_embedding: true # share in/out embeddings
ffn_dim_multiplier: 1.5 # hidden dim multiplier --> FFN size (here, 768)
flash_attn: true # use flash attn (through torch api)
compile: false # speed up training
use_autocast: true # enable mixed precision (bfloat16)

# Training
num_batches: 100000 # total steps
batch_size: 4
grad_accum_every: 4 # follows unsloth fix: https://unsloth.ai/blog/gradient 
learning_rate: 0.003
weight_decay: 0.0003
grad_clip_norm: 1.0 # gradient clipping

# Data
data_path: data/enwik8.gz
seq_len: 512

# Validation & generation
validate_every: 100
val_batches: 50
generate_every: 500
save_every: 5000
temperature: 1.0 # sampling temperature
min_p: 0.1 # min prob threshold
