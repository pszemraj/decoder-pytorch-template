# Extended test Llama configuration (matching rosa_extended.yaml)
run_dir: runs/llama_extended
seed: 42

# Model - Small Llama (same params as ROSA)
num_tokens: 256
dim: 128
depth: 4
heads: 4
dim_head: 32
tied_embedding: true
ffn_dim_multiplier: null  # will use default 8/3 ratio
flash_attn: true
compile: false

# Training - Extended test (50 batches, same as ROSA)
num_batches: 50
batch_size: 4
grad_accum_every: 2
learning_rate: 0.003
weight_decay: 0.0003
grad_clip_norm: 1.0

# Data
data_path: data/enwik8.gz
seq_len: 256

# Validation & generation
validate_every: 10
val_batches: 5
generate_every: 10
save_every: 50
temperature: 0.8
min_p: 0.05
